# ğŸ“š Books Scraper - SystÃ¨me de Veille Concurrentielle

Projet de scraping et d'analyse de donnÃ©es pour books.toscrape.com.

SystÃ¨me automatisÃ© permettant de collecter, nettoyer, stocker et exposer via API les donnÃ©es des livres du site books.toscrape.com.

## ğŸ“‹ Description du projet

Dans le cadre d'une veille concurrentielle pour une enseigne de vente de livres, ce projet met en place un systÃ¨me capable de :

- Collecter automatiquement les informations des livres (titres, prix, notes clients, disponibilitÃ©, catÃ©gories)
- Nettoyer et homogÃ©nÃ©iser les donnÃ©es (formats de prix, conversion des notes, gestion des doublons)
- Stocker les donnÃ©es dans une base relationnelle SQLite
- Exposer les donnÃ©es via une API REST
- Permettre l'analyse des donnÃ©es (prix moyens, top catÃ©gories, statistiques)

## ğŸ—ï¸ Architecture du projet

```
scrapy_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database/              # Module base de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ connection.py      # Gestion connexion SQLite
â”‚   â”‚   â””â”€â”€ book_repository.py # AccÃ¨s aux donnÃ©es (pattern Repository)
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                   # Module API REST
â”‚   â”‚   â””â”€â”€ main.py            # Application FastAPI
â”‚   â”‚
â”‚   â””â”€â”€ scraper/               # Module scraping
â”‚       â””â”€â”€ bookstoscrape_Scraper/
â”‚           â”œâ”€â”€ spiders/       # Spiders Scrapy
â”‚           â”œâ”€â”€ items.py       # DÃ©finition des donnÃ©es
â”‚           â”œâ”€â”€ pipelines.py   # Nettoyage des donnÃ©es
â”‚           â””â”€â”€ settings.py    # Configuration
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ books.db               # Base de donnÃ©es SQLite
â”‚
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â””â”€â”€ README.md                  # Documentation
```

### Principes d'architecture

Le projet respecte les principes de **Clean Code** et **Clean Architecture** :

- **SÃ©paration des responsabilitÃ©s** : Chaque module a une fonction claire (scraping, database, API)
- **OrientÃ© objet** : Utilisation de classes avec responsabilitÃ©s uniques
- **RÃ©utilisabilitÃ©** : Code modulaire et facilement testable
- **Pattern Repository** : Abstraction de l'accÃ¨s aux donnÃ©es

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8+
- pip
- Git

### Ã‰tapes d'installation

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/scrapy_project.git
cd scrapy_project

# 2. CrÃ©er un environnement virtuel
python -m venv venv

# 3. Activer l'environnement virtuel
# Sur Windows :
venv\Scripts\activate
# Sur Mac/Linux :
source venv/bin/activate

# 4. Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“Š Utilisation

### 1. Scraping des donnÃ©es

Lancer le spider pour collecter les donnÃ©es :

```bash
cd src/scraper/bookstoscrape_Scraper
scrapy crawl booktoscrape_Scraper
```

Le spider va :

- Parcourir toutes les pages du site
- Extraire les donnÃ©es de chaque livre
- Nettoyer automatiquement les donnÃ©es (prix, notes, disponibilitÃ©)
- DÃ©tecter et Ã©liminer les doublons
- Stocker dans `data/books.db`

**DurÃ©e estimÃ©e** : ~2-3 minutes pour ~1000 livres

### 2. Lancer l'API REST

Depuis la racine du projet :

```bash
uvicorn src.api.main:app --reload
```

L'API sera accessible sur : <http://localhost:8000>

**Documentation interactive** : <http://localhost:8000/docs>

### 3. RequÃªtes SQL directes

Pour des analyses personnalisÃ©es, vous pouvez interroger directement la base :

```bash
sqlite3 data/books.db

# Exemples de requÃªtes :
SELECT COUNT(*) FROM books;
SELECT category, COUNT(*) FROM books GROUP BY category;
SELECT * FROM books WHERE prix < 20 AND notation >= 4;
```

## ğŸ” API REST - Endpoints

### Endpoints principaux

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/` | Page d'accueil de l'API |
| GET | `/books` | Liste tous les livres (avec pagination) |
| GET | `/books/{id}` | DÃ©tails d'un livre spÃ©cifique |
| GET | `/books/search` | Recherche avec filtres multiples |
| GET | `/categories` | Liste toutes les catÃ©gories |
| GET | `/categories/{category}/books` | Livres d'une catÃ©gorie |
| GET | `/stats` | Statistiques globales |
| GET | `/health` | Statut de l'API |

### Exemples d'utilisation

**Lister les livres (avec pagination)**

```bash
GET http://localhost:8000/books?limit=20&offset=0
```

**Recherche avec filtres**

```bash
GET http://localhost:8000/books/search?category=Fiction&min_price=10&max_price=30&min_rating=4
```

**Statistiques globales**

```bash
GET http://localhost:8000/stats
```

RÃ©ponse exemple :

```json
{
  "global": {
    "total_livres": 1000,
    "nb_categories": 50,
    "prix_moyen": 35.67,
    "note_moyenne": 3.2,
    "stock_total": 15000
  },
  "prix_par_categorie": [...],
  "top_categories": [...]
}
```

## ğŸ—„ï¸ SchÃ©ma de la base de donnÃ©es

**Table : books**

| Colonne | Type | Description |
|---------|------|-------------|
| id | INTEGER | ClÃ© primaire auto-incrÃ©mentÃ©e |
| titre | TEXT | Titre du livre |
| prix | REAL | Prix en livres sterling (Â£) |
| notation | INTEGER | Note de 0 Ã  5 Ã©toiles |
| disponibilite | INTEGER | Nombre d'exemplaires en stock |
| description | TEXT | Description du livre |
| upc | TEXT | Code produit unique (UNIQUE) |
| category | TEXT | CatÃ©gorie du livre |
| url | TEXT | URL de la page produit |
| image | TEXT | URL de l'image de couverture |
| date_scraping | TEXT | Date/heure du scraping (ISO 8601) |

## ğŸ§¹ Pipeline de nettoyage des donnÃ©es

Le projet utilise 5 pipelines Scrapy pour garantir la qualitÃ© des donnÃ©es :

1. **CleanPricePipeline** : Convertit "Â£51.77" â†’ 51.77 (float)
2. **ConvertRatingPipeline** : Convertit "Three" â†’ 3 (int)
3. **ExtractAvailabilityPipeline** : Extrait "In stock (22 available)" â†’ 22
4. **DuplicatesPipeline** : DÃ©tecte les doublons par UPC
5. **SaveToSQLitePipeline** : Sauvegarde dans la base de donnÃ©es

## ğŸ“¦ DÃ©pendances

```
scrapy==2.13.3      # Framework de scraping
fastapi==0.115.0    # Framework API REST
uvicorn==0.32.0     # Serveur ASGI
```

## ğŸ§ª Tests

### VÃ©rifier l'intÃ©gritÃ© des donnÃ©es

```bash
# Compter les livres
sqlite3 data/books.db "SELECT COUNT(*) FROM books;"

# VÃ©rifier qu'il n'y a pas de doublons
sqlite3 data/books.db "SELECT COUNT(DISTINCT upc) FROM books;"

# Afficher quelques exemples
sqlite3 data/books.db "SELECT titre, prix, notation FROM books LIMIT 5;"
```

### Tester l'API

```bash
# Test de santÃ©
curl http://localhost:8000/health

# RÃ©cupÃ©rer des statistiques
curl http://localhost:8000/stats
```

## ğŸ› ï¸ Technologies utilisÃ©es

- **Python 3.13** : Langage principal
- **Scrapy** : Framework de web scraping
- **SQLite** : Base de donnÃ©es relationnelle
- **FastAPI** : Framework API REST moderne et rapide
- **Uvicorn** : Serveur ASGI haute performance

## ğŸ“ˆ FonctionnalitÃ©s clÃ©s

### Scraping

- Scraping rÃ©cursif avec gestion de la pagination
- Extraction de 10+ champs de donnÃ©es par livre
- Gestion automatique des URLs relatives
- Rate limiting pour respecter les serveurs

### Nettoyage

- Normalisation automatique des prix
- Conversion des notes textuelles en numÃ©riques
- Extraction intelligente de la disponibilitÃ©
- DÃ©tection des doublons par UPC

### API

- Documentation interactive OpenAPI (Swagger)
- Filtres multiples combinables
- Pagination efficace
- Gestion des erreurs HTTP appropriÃ©e
- Validation automatique des paramÃ¨tres

## ğŸ”® AmÃ©liorations futures possibles

- [ ] Migration vers PostgreSQL pour scalabilitÃ©
- [ ] Ajout de tests unitaires et d'intÃ©gration
- [ ] DÃ©ploiement sur Azure Cloud
- [ ] SystÃ¨me de cache Redis pour l'API
- [ ] Dashboard de visualisation (Plotly/Dash)
- [ ] Scraping incrÃ©mental (uniquement les nouveaux livres)
- [ ] Authentification JWT pour l'API

## ğŸ‘¤ Auteur

DÃ©veloppÃ© dans le cadre du projet "Scraping de donnÃ©es avec Scrapy" - Certification RNCP DÃ©veloppeur.se en intelligence artificielle

## ğŸ“„ Licence

Ce projet est Ã  usage Ã©ducatif uniquement.

## ğŸ“ Support

Pour toute question ou problÃ¨me :

1. Consulter la documentation interactive de l'API : `/docs`
2. VÃ©rifier les logs du scraper
3. Contacter [votre email/contact]

---

**Note** : Ce projet scrape books.toscrape.com, un site crÃ©Ã© spÃ©cifiquement pour l'apprentissage du web scraping. Respectez toujours le fichier robots.txt et les conditions d'utilisation lors du scraping de sites web.
